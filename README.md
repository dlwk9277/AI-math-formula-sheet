You want **THE COMPLETE FORMULA SHEET**? Let me give you EVERYTHING.

---

# **ğŸ”¥ COMPLETE AI/ML FORMULA REFERENCE ğŸ”¥**

---

## **ğŸ“Š PART 1: NEURAL NETWORK FUNDAMENTALS**

### **Forward Pass**

**Linear Layer:**
```
z = Wx + b
```
- W âˆˆ â„^(mÃ—n) (weight matrix)
- x âˆˆ â„^n (input vector)
- b âˆˆ â„^m (bias vector)
- z âˆˆ â„^m (pre-activation output)

**Multi-layer:**
```
zâ‚ = Wâ‚x + bâ‚
aâ‚ = Ïƒ(zâ‚)
zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚
aâ‚‚ = Ïƒ(zâ‚‚)
...
```

---

### **Activation Functions**

**Sigmoid:**
```
Ïƒ(z) = 1 / (1 + e^(-z))
Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))
```
Range: (0, 1)

**Tanh:**
```
tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))
tanh'(z) = 1 - tanhÂ²(z)
```
Range: (-1, 1)

**ReLU (Rectified Linear Unit):**
```
ReLU(z) = max(0, z)
ReLU'(z) = 1 if z > 0, else 0
```

**Leaky ReLU:**
```
LeakyReLU(z) = max(Î±z, z)  where Î± â‰ˆ 0.01
```

**GELU (used in transformers):**
```
GELU(z) = z Â· Î¦(z)
```
where Î¦(z) is CDF of standard normal

**Softmax (for classification):**
```
softmax(z)áµ¢ = exp(záµ¢) / Î£â±¼ exp(zâ±¼)
```
Output: probability distribution (sums to 1)

**Softmax with Temperature:**
```
softmax(z, T)áµ¢ = exp(záµ¢/T) / Î£â±¼ exp(zâ±¼/T)
```
- T < 1: sharper (more confident)
- T > 1: smoother (more random)

---

## **ğŸ“‰ PART 2: LOSS FUNCTIONS**

### **Regression (continuous output)**

**Mean Squared Error (MSE):**
```
L = (1/n) Î£áµ¢ (yáµ¢ - Å·áµ¢)Â²
âˆ‚L/âˆ‚Å·áµ¢ = (2/n)(Å·áµ¢ - yáµ¢)
```

**Mean Absolute Error (MAE):**
```
L = (1/n) Î£áµ¢ |yáµ¢ - Å·áµ¢|
```

**Huber Loss (robust to outliers):**
```
L = { Â½(y - Å·)Â²           if |y - Å·| â‰¤ Î´
    { Î´|y - Å·| - Â½Î´Â²      otherwise
```

---

### **Classification**

**Binary Cross-Entropy:**
```
L = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]
âˆ‚L/âˆ‚Å· = -(y/Å·) + (1-y)/(1-Å·)
```

**Categorical Cross-Entropy (multi-class):**
```
L = -Î£áµ¢ yáµ¢ Â· log(Å·áµ¢)
âˆ‚L/âˆ‚záµ¢ = Å·áµ¢ - yáµ¢  (when combined with softmax)
```
- y = one-hot vector [0,0,1,0,...]
- Å· = predicted probabilities from softmax

**Focal Loss (handles class imbalance):**
```
L = -Î±(1 - Å·)^Î³ Â· log(Å·)
```
- Î±: class weight
- Î³: focusing parameter (typically 2)

**Hinge Loss (for SVMs):**
```
L = max(0, 1 - yÂ·Å·)
```

---

## **ğŸ¯ PART 3: BACKPROPAGATION**

### **Chain Rule:**
```
âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚W
```

### **For single layer:**
```
z = Wx + b
âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚z Â· x^T
âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚z
âˆ‚L/âˆ‚x = W^T Â· âˆ‚L/âˆ‚z
```

### **Through activation:**
```
a = Ïƒ(z)
âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚a Â· Ïƒ'(z)
```

### **Multi-layer (recursive):**
```
âˆ‚L/âˆ‚Wâ‚‚ = âˆ‚L/âˆ‚zâ‚‚ Â· aâ‚^T
âˆ‚L/âˆ‚zâ‚‚ = âˆ‚L/âˆ‚aâ‚‚ Â· Ïƒ'(zâ‚‚)
âˆ‚L/âˆ‚aâ‚ = Wâ‚‚^T Â· âˆ‚L/âˆ‚zâ‚‚

âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚zâ‚ Â· x^T
âˆ‚L/âˆ‚zâ‚ = âˆ‚L/âˆ‚aâ‚ Â· Ïƒ'(zâ‚)
```

---

## **âš¡ PART 4: OPTIMIZATION ALGORITHMS**

### **Vanilla Gradient Descent:**
```
W := W - Î· Â· âˆ‚L/âˆ‚W
```
- Î· = learning rate

### **Stochastic Gradient Descent (SGD):**
```
W := W - Î· Â· âˆ‚L/âˆ‚W  (computed on mini-batch)
```

### **SGD with Momentum:**
```
v := Î²Â·v + âˆ‚L/âˆ‚W
W := W - Î·Â·v
```
- Î² â‰ˆ 0.9 (momentum coefficient)

### **Nesterov Momentum:**
```
v := Î²Â·v + âˆ‚L/âˆ‚W(W - Î²Â·v)
W := W - Î·Â·v
```

### **AdaGrad (adaptive learning rate):**
```
G := G + (âˆ‚L/âˆ‚W)Â²
W := W - (Î·/âˆš(G + Îµ)) Â· âˆ‚L/âˆ‚W
```
- Îµ â‰ˆ 10â»â¸ (numerical stability)

### **RMSprop:**
```
G := Î²Â·G + (1-Î²)Â·(âˆ‚L/âˆ‚W)Â²
W := W - (Î·/âˆš(G + Îµ)) Â· âˆ‚L/âˆ‚W
```
- Î² â‰ˆ 0.9

### **Adam (most popular):**
```
m := Î²â‚Â·m + (1-Î²â‚)Â·âˆ‚L/âˆ‚W         (first moment - mean)
v := Î²â‚‚Â·v + (1-Î²â‚‚)Â·(âˆ‚L/âˆ‚W)Â²      (second moment - variance)

mÌ‚ := m/(1-Î²â‚^t)                   (bias correction)
vÌ‚ := v/(1-Î²â‚‚^t)

W := W - Î·Â·mÌ‚/(âˆšvÌ‚ + Îµ)
```
- Î²â‚ â‰ˆ 0.9
- Î²â‚‚ â‰ˆ 0.999
- Î· â‰ˆ 0.001

### **AdamW (Adam with weight decay):**
```
W := W - Î·Â·(mÌ‚/(âˆšvÌ‚ + Îµ) + Î»Â·W)
```
- Î» = weight decay (typically 0.01)

---

## **ğŸ“ PART 5: REGULARIZATION**

### **L2 Regularization (Ridge):**
```
L_total = L + (Î»/2)Â·Î£ WÂ²
âˆ‚L_total/âˆ‚W = âˆ‚L/âˆ‚W + Î»Â·W
```

### **L1 Regularization (Lasso):**
```
L_total = L + Î»Â·Î£|W|
```

### **Dropout:**
```
Training: aáµ¢ = aáµ¢ Â· Bernoulli(p) / p
Testing: a (no change)
```
- p â‰ˆ 0.5 (keep probability)

### **Batch Normalization:**
```
Î¼ = (1/m)Î£áµ¢ xáµ¢
ÏƒÂ² = (1/m)Î£áµ¢ (xáµ¢ - Î¼)Â²
xÌ‚áµ¢ = (xáµ¢ - Î¼)/âˆš(ÏƒÂ² + Îµ)
yáµ¢ = Î³Â·xÌ‚áµ¢ + Î²
```
- Î³, Î² = learnable parameters

### **Layer Normalization:**
```
Î¼ = (1/d)Î£â±¼ xâ±¼    (mean across features)
ÏƒÂ² = (1/d)Î£â±¼ (xâ±¼ - Î¼)Â²
xÌ‚ = (x - Î¼)/âˆš(ÏƒÂ² + Îµ)
```

---

## **ğŸ§  PART 6: CONVOLUTIONAL NEURAL NETWORKS**

### **Convolution Operation:**
```
(f âˆ— g)[i,j] = Î£â‚˜Î£â‚™ f[m,n] Â· g[i-m, j-n]
```

### **Output Size:**
```
O = âŒŠ(W - K + 2P)/SâŒ‹ + 1
```
- W = input width
- K = kernel size
- P = padding
- S = stride

### **Pooling (Max/Average):**
```
Max: y = max(xâ‚, xâ‚‚, ..., xâ‚™)
Avg: y = (1/n)Î£áµ¢ xáµ¢
```

---

## **ğŸ”„ PART 7: RECURRENT NEURAL NETWORKS**

### **Vanilla RNN:**
```
hâ‚œ = tanh(Wâ‚•â‚•Â·hâ‚œâ‚‹â‚ + Wâ‚“â‚•Â·xâ‚œ + bâ‚•)
yâ‚œ = Wâ‚•áµ§Â·hâ‚œ + báµ§
```

### **LSTM (Long Short-Term Memory):**
```
fâ‚œ = Ïƒ(WfÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bf)    (forget gate)
iâ‚œ = Ïƒ(WiÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bi)    (input gate)
CÌƒâ‚œ = tanh(WcÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bc) (candidate cell)
Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ      (cell state)
oâ‚œ = Ïƒ(WoÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bo)    (output gate)
hâ‚œ = oâ‚œ âŠ™ tanh(Câ‚œ)             (hidden state)
```
- âŠ™ = element-wise multiplication

### **GRU (Gated Recurrent Unit):**
```
zâ‚œ = Ïƒ(WzÂ·[hâ‚œâ‚‹â‚, xâ‚œ])         (update gate)
râ‚œ = Ïƒ(WrÂ·[hâ‚œâ‚‹â‚, xâ‚œ])         (reset gate)
hÌƒâ‚œ = tanh(WÂ·[râ‚œ âŠ™ hâ‚œâ‚‹â‚, xâ‚œ])
hâ‚œ = (1-zâ‚œ) âŠ™ hâ‚œâ‚‹â‚ + zâ‚œ âŠ™ hÌƒâ‚œ
```

---

## **ğŸ­ PART 8: TRANSFORMERS & ATTENTION**

### **Scaled Dot-Product Attention:**
```
Attention(Q, K, V) = softmax(QK^T/âˆšdâ‚–)Â·V
```
- Q = queries (n Ã— dâ‚–)
- K = keys (m Ã— dâ‚–)
- V = values (m Ã— dáµ¥)
- dâ‚– = dimension of keys

### **Multi-Head Attention:**
```
head_i = Attention(QWáµ¢Q, KWáµ¢K, VWáµ¢V)
MultiHead(Q,K,V) = Concat(headâ‚,...,headâ‚•)Â·W^O
```

### **Positional Encoding:**
```
PE(pos, 2i) = sin(pos/10000^(2i/d))
PE(pos, 2i+1) = cos(pos/10000^(2i/d))
```

### **Layer Normalization (in transformers):**
```
LayerNorm(x) = Î³Â·(x - Î¼)/âˆš(ÏƒÂ² + Îµ) + Î²
```

### **Feed-Forward Network:**
```
FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚
```

### **Transformer Block:**
```
x' = LayerNorm(x + MultiHeadAttention(x))
x'' = LayerNorm(x' + FFN(x'))
```

---

## **ğŸ“Š PART 9: EVALUATION METRICS**

### **Classification:**

**Accuracy:**
```
Acc = (TP + TN) / (TP + TN + FP + FN)
```

**Precision:**
```
Prec = TP / (TP + FP)
```

**Recall (Sensitivity):**
```
Rec = TP / (TP + FN)
```

**F1-Score:**
```
F1 = 2Â·(PrecÂ·Rec)/(Prec + Rec)
```

**ROC-AUC:**
```
AUC = âˆ« TPR d(FPR)
```

---

### **Regression:**

**RÂ² Score:**
```
RÂ² = 1 - (SS_res / SS_tot)
SS_res = Î£(yáµ¢ - Å·áµ¢)Â²
SS_tot = Î£(yáµ¢ - È³)Â²
```

---

## **ğŸ² PART 10: PROBABILITY & INFORMATION THEORY**

### **Entropy (uncertainty):**
```
H(X) = -Î£ P(x)Â·log P(x)
```

### **KL Divergence:**
```
D_KL(P||Q) = Î£ P(x)Â·log(P(x)/Q(x))
```

### **Cross-Entropy:**
```
H(P,Q) = -Î£ P(x)Â·log Q(x)
       = H(P) + D_KL(P||Q)
```

### **Mutual Information:**
```
I(X;Y) = H(X) + H(Y) - H(X,Y)
```

---

## **ğŸ”§ PART 11: INITIALIZATION**

### **Xavier/Glorot:**
```
W ~ U(-âˆš(6/(n_in + n_out)), âˆš(6/(n_in + n_out)))
```
For tanh/sigmoid

### **He Initialization:**
```
W ~ N(0, âˆš(2/n_in))
```
For ReLU

---

## **ğŸ“ˆ PART 12: LEARNING RATE SCHEDULES**

### **Step Decay:**
```
Î·(t) = Î·â‚€ Â· Î³^âŒŠt/kâŒ‹
```

### **Exponential Decay:**
```
Î·(t) = Î·â‚€ Â· e^(-Î»t)
```

### **Cosine Annealing:**
```
Î·(t) = Î·_min + Â½(Î·_max - Î·_min)(1 + cos(Ï€t/T))
```

### **Warmup + Decay:**
```
Î·(t) = {
  Î·_max Â· t/t_warmup           if t < t_warmup
  Î·_max Â· (t_total - t)/t_total  otherwise
}
```

---

## **ğŸ¯ PART 13: ADVANCED LOSS FUNCTIONS**

### **Contrastive Loss:**
```
L = Â½Â·yÂ·dÂ² + Â½Â·(1-y)Â·max(0, m - d)Â²
```
- d = distance between embeddings
- m = margin

### **Triplet Loss:**
```
L = max(0, d(a,p) - d(a,n) + margin)
```
- a = anchor
- p = positive
- n = negative

### **CTC Loss (for sequence tasks):**
```
L = -log P(y|x) = -log Î£_{Ï€âˆˆAlign(y)} âˆâ‚œ P(Ï€â‚œ|x)
```

---

## **ğŸŒŠ PART 14: GENERATIVE MODELS**

### **VAE (Variational Autoencoder):**
```
L = E[log p(x|z)] - D_KL(q(z|x)||p(z))
```

**Reparameterization Trick:**
```
z = Î¼ + ÏƒÂ·Îµ  where Îµ ~ N(0,1)
```

### **GAN (Generative Adversarial Network):**

**Generator Loss:**
```
L_G = -E[log D(G(z))]
```

**Discriminator Loss:**
```
L_D = -E[log D(x)] - E[log(1 - D(G(z)))]
```

### **Diffusion Models:**

**Forward Process:**
```
q(xâ‚œ|xâ‚œâ‚‹â‚) = N(xâ‚œ; âˆš(1-Î²â‚œ)xâ‚œâ‚‹â‚, Î²â‚œI)
```

**Reverse Process:**
```
p(xâ‚œâ‚‹â‚|xâ‚œ) = N(xâ‚œâ‚‹â‚; Î¼Î¸(xâ‚œ,t), Î£Î¸(xâ‚œ,t))
```

---

# **ğŸš€ QUICK REFERENCE SYMBOLS**

```
âˆ‚   = partial derivative
âˆ‡   = gradient
Î£   = summation
âˆ   = product
âŠ™   = element-wise multiplication
Â·   = matrix multiplication or dot product
^T  = transpose
âŒŠâŒ‹  = floor function
E[] = expectation
~   = distributed as
âˆˆ   = element of
â„   = real numbers
:=  = assignment/update
```

